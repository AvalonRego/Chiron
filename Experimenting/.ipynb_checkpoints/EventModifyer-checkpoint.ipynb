{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba0085-6673-4be9-8d6b-a5cdeada059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"hdf5_processing_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def process_and_save_hdf5(file_path):\n",
    "    \"\"\"Processes and modifies an HDF5 file in-place.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logging.info(f\"Processing {file_path}\")\n",
    "        print(file_path)\n",
    "\n",
    "        name=os.path.basename(file_path)\n",
    "        num,_=name.split('.')\n",
    "        with pd.HDFStore(file_path, mode='a') as store:  # 'a' mode allows modifying the file\n",
    "            if 'hits' not in store or 'records' not in store:\n",
    "                logging.error(f\"Skipping {file_path}: Missing 'hits' or 'records' dataset.\")\n",
    "                return f\"Skipped {file_path}: Missing required datasets.\"\n",
    "\n",
    "            logging.info('Successfull loading')\n",
    "            # Read datasets\n",
    "            hits = store['hits']\n",
    "            records = store['records']\n",
    "\n",
    "            if 'event_no' in hits.columns:\n",
    "                hits = hits.drop(columns=['event_no'])\n",
    "            if 'event_no' in records.columns:\n",
    "                records = records.drop(columns=['event_no'])\n",
    "\n",
    "            # Compute event bins per record_id (Optimized)\n",
    "            hits[\"min_time\"] = hits.groupby(\"record_id\")[\"time\"].transform('min')\n",
    "            hits[\"event_no\"] = np.floor((hits[\"time\"] - hits[\"min_time\"]) // 100 + 1).astype(np.int64)\n",
    "            hits[\"event_no\"] += ( hits[\"record_id\"].astype(np.int64) * 10**6  # Ensures uniqueness within a dataset\n",
    "                                    + np.int64(num) * 10**12 ) # Adds uniqueness across datasets)\n",
    "\n",
    "            hits.drop(columns=[\"min_time\"], inplace=True)\n",
    "\n",
    "            # Generate event truth mapping (Corrected Aggregation)\n",
    "            event_truth = hits[['record_id', 'event_no', 'type']].drop_duplicates()\n",
    "            event_truth = event_truth.groupby('event_no', as_index=False).agg({'record_id': 'min', 'type': 'min'})\n",
    "\n",
    "            # Drop 'type' column before merging\n",
    "            #records.drop(columns=['type'], errors='ignore', inplace=True)\n",
    "            #hits.drop(columns=['type'], errors='ignore', inplace=True)\n",
    "\n",
    "            # Merge on event_no (Corrected)\n",
    "            merged_df = records.merge(event_truth, on='record_id', how='left', sort=False)\n",
    "            return hits,merged_df\n",
    "            logging.info('hit the save part of the function')\n",
    "            # âœ… Overwrite modified datasets inside the same HDF5 file\n",
    "            store.put('hits', hits, format='table', data_columns=True)\n",
    "            store.put('records', merged_df, format='table', data_columns=True)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        logging.info(f\"Completed {file_path} in {duration:.2f} seconds\")\n",
    "        return f\"Processed {file_path} in {duration:.2f} seconds\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return f\"Error processing {file_path}: {str(e)}\"\n",
    "\n",
    "def update_progress_bar(total_files, progress_queue):\n",
    "    \"\"\"Updates the progress bar based on completed tasks.\"\"\"\n",
    "    with tqdm(total=total_files, desc=\"Processing HDF5 Files\") as pbar:\n",
    "        for _ in range(total_files):\n",
    "            progress_queue.get()  # Wait for an update\n",
    "            pbar.update(1)\n",
    "\n",
    "def process_files_parallel(files, num_workers=8):\n",
    "    \"\"\"Processes multiple HDF5 files in parallel using multiprocessing.\"\"\"\n",
    "    manager = multiprocessing.Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    total_files = len(files)\n",
    "\n",
    "    # Start progress bar in a separate process\n",
    "    progress_process = multiprocessing.Process(target=update_progress_bar, args=(total_files, progress_queue))\n",
    "    progress_process.start()\n",
    "\n",
    "    def update_progress(_):\n",
    "        \"\"\"Callback function to update progress bar after each process.\"\"\"\n",
    "        progress_queue.put(1)\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        for file_path in files:\n",
    "            pool.apply_async(process_and_save_hdf5, args=(file_path,), callback=update_progress)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    progress_process.join()\n",
    "    print(\"Processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad23a1f-f7a8-44e4-887d-0a004eaa2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits,records=process_and_save_hdf5('data/LargeTMerge/0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85a5ef-1d08-44e1-8d33-db1bbd6ae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits[hits['type']!=2]['event_no'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57bb34-6934-4009-b867-7332a0917c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(hits,filter_value):\n",
    "    # Use `.value_counts(sort=False)` to prevent unnecessary sorting overhead\n",
    "    event_counts = hits['event_no'].value_counts(sort=False)\n",
    "    filtered_counts = hits.loc[hits['type'] != 2, 'event_no'].value_counts(sort=False)\n",
    "\n",
    "    # Align indices efficiently without introducing NaNs\n",
    "    filtered_counts = filtered_counts.reindex(event_counts.index, fill_value=0).astype(int)\n",
    "\n",
    "    # Vectorized filtering (avoids loops for large datasets)\n",
    "    valid_events = event_counts.index[event_counts.values <= filter_value * (filtered_counts.values+1)]\n",
    "\n",
    "    # Use `.loc` for efficient filtering\n",
    "    return hits.loc[hits['event_no'].isin(valid_events)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae23ff8-7d5f-4468-8b9d-8fb442940fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=filter_events(hits,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681526d6-eca1-4d7e-b9ca-01673154fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=hits['event_no'].unique()\n",
    "v=a['event_no'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453ec0f-2d84-4dc3-a7bd-40447beed97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[i for i in u if i not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29f502-7541-48e5-99f4-4ec19ccd6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb69d89-4e36-439c-9b87-e98785c73d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c2929-d06d-4eeb-aaf2-e217b7538068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27942ae-fa3c-438b-8bf3-03c657f67b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot(hitss):\n",
    "    \"\"\"\n",
    "    Generate and display a histogram plot of 'time' for different 'type' values \n",
    "    within a specific 'record_id' in the given DataFrame.\n",
    "    \"\"\"\n",
    "    info={\n",
    "        0:'Realistic Tracks',\n",
    "        1:'Cascades',\n",
    "        2:'Starting Track',\n",
    "        20:'Electrical noise',\n",
    "        21:'biolumi noise',\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Select the first unique record_id\n",
    "    records = hitss['record_id'].unique()\n",
    "    for record in records:\n",
    "        \n",
    "            # Create a figure\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for i in range(5):\n",
    "            # Get all unique types\n",
    "            hits=filter_events(hitss,i)\n",
    "            types = hits['type'].unique()\n",
    "\n",
    "            # Filter hits for the selected record_id\n",
    "            hit = hits[hits['record_id'] == record]\n",
    "\n",
    "\n",
    "            interval = np.arange(hit['time'].min(), hit['time'].max(), 100)\n",
    "\n",
    "            y,x = np.histogram(hit[hit['type'] == 2]['time'], bins=interval)\n",
    "            x = (x[:-1] + x[1:]) / 2  # Convert bin edges to bin centers\n",
    "            ax.plot(x, y+1, label=f'Type : hits filter level{i}')\n",
    "\n",
    "#             y,x = np.histogram(hit[hit['type'] != 2]['time'], bins=interval)\n",
    "#             x = (x[:-1] + x[1:]) / 2  # Convert bin edges to bin centers\n",
    "#             ax.plot(x, y+1, label=f'Type : Noise')\n",
    "\n",
    "            # y, x = np.histogram(hit['time'], bins=interval)\n",
    "            # x = (x[:-1] + x[1:]) / 2  # Convert bin edges to bin centers\n",
    "            # ax.plot(x, y+1, label=f'Type : All filter level {i}')\n",
    "\n",
    "            # Labeling and legend\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.set_title(f\"Hit histogram for Record ID {record}\")\n",
    "            ax.legend()\n",
    "\n",
    "            plt.xlim([0,5000])\n",
    "            # Show the plot each time the function is called\n",
    "            plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "#plot(hits)  # This will display the plot each time the function is called\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab53b6-89a9-44a3-9d40-208335c85076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562af87-a66a-441d-bac5-739f19cec2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympus",
   "language": "python",
   "name": "olympus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
